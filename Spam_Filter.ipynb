{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a spam filter with Naive Bayes algorithm.\n",
    "\n",
    "In this project, we're going to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm.\n",
    "\n",
    "To train the algorithm, we'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by *Tiago A. Almeida and José María Gómez Hidalgo*, and it can be downloaded from the The [University of California at Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection).\n",
    "\n",
    "This project is based on and extends a similar project shared by [mircealex @ DataQuest](https://github.com/dataquestio/solutions/blob/master/Mission433Solutions.ipynb).\n",
    "\n",
    "We will use **pandas** to manipluate the data. A multinomial naive Bayes algorithm is used and the mathematical details are given below in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "First we read in the dataset and do basic checks - size, head, tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "  Label                                                SMS\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "     Label                                                SMS\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               Will ü b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n"
     ]
    }
   ],
   "source": [
    "sms_data = pd.read_csv('data/SMSSpamCollection', sep='\\t', header=None, names = ['Label', 'SMS'])\n",
    "print(sms_data.shape)\n",
    "print( sms_data.head() )\n",
    "print( sms_data.tail() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the head/tail we see that most messages are labelled as ham, some are spam. This is normal. Let's check the exact percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that 86.6% messages are ham, and 13.4% are spam. This sample looks representative based on common experience.\n",
    "\n",
    "# Working with the data\n",
    "## Training/Test split\n",
    "Now we split the data into training and test sets (the usual 80/20 split). 80% for the training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data is  (4458, 2)  and the shape of test data is  (1114, 2)\n",
      "Training set: \n",
      " Label\n",
      "ham     0.86541\n",
      "spam    0.13459\n",
      "Name: proportion, dtype: float64\n",
      "Test set: \n",
      " Label\n",
      "ham     0.868043\n",
      "spam    0.131957\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# First randomise the dataset\n",
    "data_randomised = sms_data.sample(frac = 1, random_state= 1)\n",
    "\n",
    "# Calculate index for split at 80% of data\n",
    "split_index = round(len(data_randomised) * 0.8 )\n",
    "\n",
    "# Training/ test split at the split_index\n",
    "training_data = data_randomised[ : split_index ].reset_index(drop=True)\n",
    "test_data = data_randomised[ split_index: ].reset_index(drop=True)\n",
    "\n",
    "print(\"Shape of training data is \", training_data.shape, \" and the shape of test data is \", test_data.shape)\n",
    "\n",
    "# Now do a sanity check to see that both sets of data has the ham/spam ratio of the original data.\n",
    "print (\"Training set: \\n\", training_data['Label'].value_counts(normalize=True) )\n",
    "print (\"Test set: \\n\", test_data['Label'].value_counts(normalize=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratios look similar to the original dataset, which his good. We now move on to clean the dataset.\n",
    "\n",
    "## Data cleaning\n",
    "In order to work with the algorithm for calculating probabilities, we need to convert all the text messages to see the number of keywords. The below image captures the essence of what we need to do.\n",
    "\n",
    "![](data_cleaning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation and convert everything to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before cleaning: \n",
      "   Label                                                SMS\n",
      "0   ham                       Yep, by the pretty sculpture\n",
      "1   ham      Yes, princess. Are you going to make me moan?\n",
      "2   ham                         Welp apparently he retired\n",
      "3   ham                                            Havent.\n",
      "4   ham  I forgot 2 ask ü all smth.. There's a card on ...\n",
      "Data after cleaning: \n",
      "   Label                                                SMS\n",
      "0   ham                       yep  by the pretty sculpture\n",
      "1   ham      yes  princess  are you going to make me moan \n",
      "2   ham                         welp apparently he retired\n",
      "3   ham                                            havent \n",
      "4   ham  i forgot 2 ask ü all smth   there s a card on ...\n"
     ]
    }
   ],
   "source": [
    "# Remove all the punctuation and bring all letters to lower case\n",
    "print(\"Data before cleaning: \\n\", training_data.head())\n",
    "\n",
    "training_data['SMS'] = training_data['SMS'].str.replace(r'\\W', ' ', regex=True) # replace any non-word characters with a space\n",
    "training_data['SMS'] = training_data['SMS'].str.lower() # convert everything to lower case\n",
    "\n",
    "print(\"Data after cleaning: \\n\", training_data.head())\n",
    "\n",
    "# print(training_data['SMS'].str.replace(r'\\W', ' ', regex=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of all the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in our vocabulary is 7783\n"
     ]
    }
   ],
   "source": [
    "training_data['SMS'] = training_data['SMS'].str.split() # this converts SMS column into a list of all the words\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for sms in training_data['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "# Currently vocabulary contains all the words in the dataset. Remove duplicates:\n",
    "vocabulary = set(vocabulary)\n",
    "print(\"The total number of words in our vocabulary is\", len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doit</th>\n",
       "      <th>unintentional</th>\n",
       "      <th>everyday</th>\n",
       "      <th>den</th>\n",
       "      <th>christmas</th>\n",
       "      <th>unspoken</th>\n",
       "      <th>def</th>\n",
       "      <th>remain</th>\n",
       "      <th>bcs</th>\n",
       "      <th>secret</th>\n",
       "      <th>...</th>\n",
       "      <th>jada</th>\n",
       "      <th>doubletxt</th>\n",
       "      <th>shah</th>\n",
       "      <th>agree</th>\n",
       "      <th>december</th>\n",
       "      <th>tddnewsletter</th>\n",
       "      <th>offered</th>\n",
       "      <th>6zf</th>\n",
       "      <th>carefully</th>\n",
       "      <th>tix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doit  unintentional  everyday  den  christmas  unspoken  def  remain  bcs  \\\n",
       "0     0              0         0    0          0         0    0       0    0   \n",
       "1     0              0         0    0          0         0    0       0    0   \n",
       "2     0              0         0    0          0         0    0       0    0   \n",
       "3     0              0         0    0          0         0    0       0    0   \n",
       "4     0              0         0    0          0         0    0       0    0   \n",
       "\n",
       "   secret  ...  jada  doubletxt  shah  agree  december  tddnewsletter  \\\n",
       "0       0  ...     0          0     0      0         0              0   \n",
       "1       0  ...     0          0     0      0         0              0   \n",
       "2       0  ...     0          0     0      0         0              0   \n",
       "3       0  ...     0          0     0      0         0              0   \n",
       "4       0  ...     0          0     0      0         0              0   \n",
       "\n",
       "   offered  6zf  carefully  tix  \n",
       "0        0    0          0    0  \n",
       "1        0    0          0    0  \n",
       "2        0    0          0    0  \n",
       "3        0    0          0    0  \n",
       "4        0    0          0    0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First initialise a dictionary with zeros. Each word corresponds to a list of numbers of how many times it appears in each sms.\n",
    "word_count_per_sms = {unique_word: [0]* len(training_data['SMS']) for unique_word in vocabulary }\n",
    "\n",
    "# This loop assigns the current frequency of each word.\n",
    "for index, sms in enumerate(training_data['SMS']):\n",
    "    for word in sms:\n",
    "        word_count_per_sms[word][index] +=1 \n",
    "\n",
    "# Now convert this dictionary to a dataframe\n",
    "word_counts = pd.DataFrame(word_count_per_sms)\n",
    "word_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>doit</th>\n",
       "      <th>unintentional</th>\n",
       "      <th>everyday</th>\n",
       "      <th>den</th>\n",
       "      <th>christmas</th>\n",
       "      <th>unspoken</th>\n",
       "      <th>def</th>\n",
       "      <th>remain</th>\n",
       "      <th>...</th>\n",
       "      <th>jada</th>\n",
       "      <th>doubletxt</th>\n",
       "      <th>shah</th>\n",
       "      <th>agree</th>\n",
       "      <th>december</th>\n",
       "      <th>tddnewsletter</th>\n",
       "      <th>offered</th>\n",
       "      <th>6zf</th>\n",
       "      <th>carefully</th>\n",
       "      <th>tix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  doit  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]     0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...     0   \n",
       "2   ham                    [welp, apparently, he, retired]     0   \n",
       "3   ham                                           [havent]     0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...     0   \n",
       "\n",
       "   unintentional  everyday  den  christmas  unspoken  def  remain  ...  jada  \\\n",
       "0              0         0    0          0         0    0       0  ...     0   \n",
       "1              0         0    0          0         0    0       0  ...     0   \n",
       "2              0         0    0          0         0    0       0  ...     0   \n",
       "3              0         0    0          0         0    0       0  ...     0   \n",
       "4              0         0    0          0         0    0       0  ...     0   \n",
       "\n",
       "   doubletxt  shah  agree  december  tddnewsletter  offered  6zf  carefully  \\\n",
       "0          0     0      0         0              0        0    0          0   \n",
       "1          0     0      0         0              0        0    0          0   \n",
       "2          0     0      0         0              0        0    0          0   \n",
       "3          0     0      0         0              0        0    0          0   \n",
       "4          0     0      0         0              0        0    0          0   \n",
       "\n",
       "   tix  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now merge the above with the modified training set dataframe to get the final dataset to work with\n",
    "training_data_clean = pd.concat([training_data, word_counts], axis= 1)\n",
    "training_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the Multinomial Naive Bayes algorithm\n",
    "\n",
    "For each new message, we need to calculate the following probabilities to classify as spam or ham.\n",
    "\n",
    "\\begin{equation}\n",
    "P(Spam| w_1, w_2, \\dotsc , w_n) \\propto P (Spam)  \\prod\\limits_{i=1}^{n} P(w_i | Spam )\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Ham| w_1, w_2, \\dotsc , w_n) \\propto P (Ham)  \\prod\\limits_{i=1}^{n} P(w_i | Ham )\n",
    "\\end{equation}\n",
    "\n",
    "In order to calculate the conditional probabilities $ P(w_i | Spam )$ and $P(w_i | Ham )$, we will use the following standard formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i | Spam ) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "\\end{equation}\n",
    "\n",
    "In the above:\n",
    "\n",
    "- $N_{w_i|Spam}$ = number of times the word $w_i$ appears in spam messages.\n",
    "- $N_{w_i|Ham}$ = number of times the word $w_i$ appears in ham messages.\n",
    "- $N_{Spam}$ = total number of words in the Spam messages.\n",
    "- $N_{Ham}$ = total number of words in the Ham messages.\n",
    "- $N_{Spam}$ = number of words in the total vocabulary.\n",
    "- $\\alpha$ = Laplace smoothing parameter (default is 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use our training set to calculate $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$, $P (Spam)$ and $P (Ham)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The obtained variables are: P(Spam) = 0.13 , P(Ham) =  0.87 , N_Spam = 15190 , N_Ham = 57237 , N_Vocabulary = 7783\n"
     ]
    }
   ],
   "source": [
    "# Isolate spam and ham messages into different datasets\n",
    "spam_data = training_data_clean[training_data_clean['Label'] == 'spam']\n",
    "ham_data = training_data_clean[training_data_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_data)/ len(training_data_clean)\n",
    "p_ham = 1 - p_spam\n",
    "\n",
    "# N_Spam\n",
    "n_per_spam = spam_data['SMS'].apply(len)\n",
    "n_spam = n_per_spam.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_per_ham = ham_data['SMS'].apply(len)\n",
    "n_ham = n_per_ham.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing default value\n",
    "alpha = 1\n",
    "\n",
    "print(\"The obtained variables are: P(Spam) =\", round(p_spam, 2), \", P(Ham) = \", round(p_ham, 2), \n",
    "      \", N_Spam =\", n_spam, \", N_Ham =\", n_ham, \", N_Vocabulary =\", n_vocabulary) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probabilities\n",
    "\n",
    "Having calculated the variables above, we can now calculate the conditional probability  $ P(w_i | Spam )$ and $P(w_i | Ham )$ for each word using the formulas stated above. The only new calculation is to find $N_{w_i|Spam}$ and $N_{w_i|Ham}$ for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the variables for conditional probabilities in a dictionary data structure.\n",
    "probabilities_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "probabilities_ham = {unique_word: 0 for unique_word in vocabulary}\n",
    "\n",
    "# Assign the actual values\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_data[word].sum()\n",
    "    probabilities_spam[word] = ( n_word_given_spam + alpha ) / ( n_spam + alpha* n_vocabulary )\n",
    "\n",
    "    n_word_given_ham = ham_data[word].sum()\n",
    "    probabilities_ham[word] = ( n_word_given_ham + alpha ) / ( n_ham + alpha* n_vocabulary )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying a new message\n",
    "\n",
    "Now that all the background variables are calculated, we can start creating the spam filter. The function works as follows:\n",
    "\n",
    "- Input a new message $(w_1, w_2, \\dotsc , w_n)$.\n",
    "- Calculate $P(Spam | w_1, w_2, , \\dotsc, w_n)$ and $P(Ham | w_1, w_2, , \\dotsc, w_n)$ using the formulas listed above.\n",
    "- Use the above probabilities to classify as ham or spam.\n",
    "    - Ham if $P(Spam | w_1, w_2, , \\dotsc, w_n) < P(Ham | w_1, w_2, , \\dotsc, w_n)$\n",
    "    - Spam if $P(Spam | w_1, w_2, , \\dotsc, w_n) > P(Ham | w_1, w_2, , \\dotsc, w_n)$\n",
    "    - Undecided if $P(Spam | w_1, w_2, , \\dotsc, w_n) = P(Ham | w_1, w_2, , \\dotsc, w_n)$ (although this is expected to be quite rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier function\n",
    "def classify(sms):\n",
    "    '''\n",
    "    Input the sms as a string.\n",
    "    '''\n",
    "\n",
    "    # Remove punctuation and convert to lower case\n",
    "    sms = re.sub('\\W', ' ', sms)\n",
    "    sms = sms.lower().split()\n",
    "\n",
    "    # initialise the probabilities\n",
    "    p_spam_given_sms = p_spam\n",
    "    p_ham_given_sms = p_ham\n",
    "\n",
    "    for word in sms:\n",
    "        if word in probabilities_spam:\n",
    "            p_spam_given_sms *= probabilities_spam[word]\n",
    "        \n",
    "        if word in probabilities_ham:\n",
    "            p_ham_given_sms *= probabilities_ham[word]\n",
    "\n",
    "    print('P(Spam|sms) =', p_spam_given_sms, 'and P(Ham|sms) =', p_ham_given_sms)\n",
    "    \n",
    "    if p_ham_given_sms > p_spam_given_sms:\n",
    "        print('Label: Ham')\n",
    "    elif p_spam_given_sms > p_ham_given_sms:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal probabilities. A human should classify this.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|sms) = 7.538598683744844e-19 and P(Ham|sms) = 5.625552349491667e-14\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify('Alright then, see you later')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|sms) = 9.654365138458546e-22 and P(Ham|sms) = 5.159017989360921e-25\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('Winner!! Click here to win the prize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|sms) = 8.007841742970287e-21 and P(Ham|sms) = 2.6533008318723246e-21\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify('You have been chosen for the jackpot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the filter\n",
    "\n",
    "The filter works well for the manual examples above. But let's define a proper way to assess its accuracy. Modify the above function to return a classification label rather than printing the result. Then apply that on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier function\n",
    "def classify_test_data(sms):\n",
    "    '''\n",
    "    Input the sms as a string.\n",
    "    '''\n",
    "\n",
    "    # Remove punctuation and convert to lower case\n",
    "    sms = re.sub('\\W', ' ', sms)\n",
    "    sms = sms.lower().split()\n",
    "\n",
    "    # initialise the probabilities\n",
    "    p_spam_given_sms = p_spam\n",
    "    p_ham_given_sms = p_ham\n",
    "\n",
    "    for word in sms:\n",
    "        if word in probabilities_spam:\n",
    "            p_spam_given_sms *= probabilities_spam[word]\n",
    "        \n",
    "        if word in probabilities_ham:\n",
    "            p_ham_given_sms *= probabilities_ham[word]\n",
    "\n",
    "    # print('P(Spam|sms) =', p_spam_given_sms, 'and P(Ham|sms) =', p_ham_given_sms)\n",
    "    \n",
    "    if p_ham_given_sms > p_spam_given_sms:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_sms > p_ham_given_sms:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'undecided'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predictions\n",
       "0   ham          Later i guess. I needa do mcat study too.         ham\n",
       "1   ham             But i haf enuff space got like 4 mb...         ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...        spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...         ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...         ham"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the above function on the test set to create a new column of predicted values\n",
    "test_data['predictions'] = test_data['SMS'].apply(classify_test_data)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct predictions: 1100 out of a total 1114 messages.\n",
      "Accuracy is 98.74 %.\n"
     ]
    }
   ],
   "source": [
    "# Measure the accuracy\n",
    "\n",
    "correct_predictions = 0\n",
    "total_rows = test_data.shape[0]\n",
    "\n",
    "for _ , row in test_data.iterrows():\n",
    "    if row['Label'] == row['predictions']:\n",
    "        correct_predictions +=1\n",
    "\n",
    "print (\"Number of correct predictions:\", correct_predictions, \"out of a total\", total_rows, \"messages.\")\n",
    "print (\"Accuracy is\", round( 100* correct_predictions/ total_rows , 2), \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is over 98% which is pretty good. Now we look at the messages below to see where did our classifier fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages incorrectly flagged as spam: \n",
      "\n",
      "Unlimited texts. Limited minutes.\n",
      "26th OF JULY\n",
      "Nokia phone is lovly..\n",
      "A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked 'hw cn u run so fast?' D boy replied \"Boost is d secret of my energy\" n instantly d girl shouted \"our energy\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8\n",
      "No calls..messages..missed calls\n",
      "We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us\n",
      "\n",
      " \n",
      " Spam messages that were missed: \n",
      "\n",
      "Not heard from U4 a while. Call me now am here all night with just my knickers on. Make me beg for it like U did last time 01223585236 XX Luv Nikiyu4.net\n",
      "More people are dogging in your area now. Call 09090204448 and join like minded guys. Why not arrange 1 yourself. There's 1 this evening. A£1.50 minAPN LS278BB\n",
      "Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50\n",
      "Hi babe its Chloe, how r u? I was smashed on saturday night, it was great! How was your weekend? U been missing me? SP visionsms.com Text stop to stop 150p/text\n",
      "0A$NETWORKS allow companies to bill for SMS, so they are responsible for their \"suppliers\", just as a shop has to give a guarantee on what they sell. B. G.\n",
      "RCT' THNQ Adrian for U text. Rgds Vatian\n",
      "2/2 146tf150p\n",
      "Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r\n"
     ]
    }
   ],
   "source": [
    "# Print the ham messages that we classified incorrectly.\n",
    "\n",
    "print (\"Messages incorrectly flagged as spam: \\n\")\n",
    "for _, row in test_data.iterrows():\n",
    "    if row['Label'] != row['predictions'] and row['Label'] == 'ham':\n",
    "        print(row['SMS'])\n",
    "\n",
    "print (\"\\n \\n Spam messages that were missed: \\n\")\n",
    "for _, row in test_data.iterrows():\n",
    "    if row['Label'] != row['predictions'] and row['Label'] == 'spam':\n",
    "        print(row['SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another definition of accuracy\n",
    "Another way to analyse the accuracy is by looking at the percentage of spam messages the filter misses and the percentage of ham messages it mislabels and blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of a total 967 ham messages, 961 were identified correctly.\n",
      "of a total 147 spam messages, 139 were identified correctly.\n",
      "Sensitivity of the filter is 94.56 percent.\n",
      "Specificity of the filter is 99.38 percent.\n"
     ]
    }
   ],
   "source": [
    "# number of ham/spam in test data\n",
    "num_ham_test = test_data['Label'].value_counts()['ham']\n",
    "num_spam_test = test_data['Label'].value_counts()['spam']\n",
    "\n",
    "num_ham_correct_pred = 0\n",
    "num_spam_correct_pred = 0\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    if row['Label'] == 'ham':\n",
    "        if row['predictions'] == 'ham':\n",
    "            num_ham_correct_pred += 1\n",
    "    elif row['Label'] == 'spam':\n",
    "        if row['predictions'] == 'spam':\n",
    "            num_spam_correct_pred += 1\n",
    "\n",
    "print (\"Of a total\", num_ham_test, \"ham messages,\", num_ham_correct_pred, \"were identified correctly.\")\n",
    "print (\"of a total\", num_spam_test, \"spam messages,\", num_spam_correct_pred, \"were identified correctly.\" )\n",
    "\n",
    "# Calculate the sensitivity and specificity of the test.\n",
    "# Sensitivity = probability of the message being declared a spam given that it is a spam.\n",
    "print (\"Sensitivity of the filter is\", round (100* num_spam_correct_pred/ num_spam_test, 2), \"%.\")\n",
    "\n",
    "# Specificity = probability of the message being declared ham given that it is ham.\n",
    "print (\"Specificity of the filter is\", round (100* num_ham_correct_pred/ num_ham_test, 2), \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve...\n",
    "\n",
    "Accuracy is currently over 98% but it can also be improved further. \n",
    "\n",
    "The following parameters have been manually used in the entire code and changing them changes the final result:\n",
    "- percentage of data split between training and test set (currently 80%)\n",
    "- random_state while randomising the data (current value = 1)\n",
    "- Laplace smoothing parameter (current value = 1)\n",
    "\n",
    "Other possibilities:\n",
    "- Would it be useful to keep upper and lower cases in the messages? It will increase the size of vocabulary and may make the algorithm more accurate.\n",
    "- More complex algorithm using Machine Learning. Currently we're just looking at individual word frequencies without any context. Using a more refined NLP code will make this better - although that's a completely different project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
